{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qj2MHAH_5cT5"
   },
   "source": [
    "## Emojify!\n",
    "Welcome to the third and last programming assignment! You are going to use word vector representations to build an Emojifier.\n",
    "\n",
    "You will implement a model which inputs a sentence (such as \"Let's go see the baseball game tonight!\") and finds the most appropriate emoji to be used with this sentence (撅).\n",
    "\n",
    "By using word vectors, you'll see that even if your training set explicitly relates only a few words to a particular emoji, your algorithm will be able to generalize and associate words in the test set to the same emoji even if those words don't even appear in the training set. This allows you to build an accurate classifier mapping from sentences to emojis, even using a small training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De1sZfiz5cT9"
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKZY_w-e5cT-"
   },
   "source": [
    "Let's first import all the packages that you will need during this part of assignment.\n",
    "\n",
    "Feel free to use other libraries if you want to.\n",
    "\n",
    "If you don't have emoji or other libraries, write \"pip install emoji\" command in one of the code cells in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1671715133204,
     "user": {
      "displayName": " 专抓",
      "userId": "09515891463711318510"
     },
     "user_tz": -120
    },
    "id": "1R-OcY6e5cT-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzuBgLh-5cT_"
   },
   "source": [
    "## Import and visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7o-wxwk5cUA"
   },
   "source": [
    "In this part you need to:\n",
    "1. Import the train and test data\n",
    "2. Seperate the sentences (in the first column) and the index of the emoji (in the second column).\n",
    "3. Convert the Y value of every sentence from emoji index (0-4) to one hot encoding. 2 --> [0,0,1,0,0]\n",
    "4. Print 10 sentances from training data and visualize their matching emojies using the label_to_emoji() help function. Print also the one-hot-encoding representation of these sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1671715135648,
     "user": {
      "displayName": " 专抓",
      "userId": "09515891463711318510"
     },
     "user_tz": -120
    },
    "id": "x_mp0gfs5cUA"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "# import data\n",
    "train = pd.read_csv('train_emoji.csv', names=[\"sentence\", \"emoji\"])\n",
    "test = pd.read_csv('test_emoji.csv', names=[\"sentence\", \"emoji\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French macaroon is so tasty</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>work is horrible</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am upset</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>throw the ball</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      sentence  emoji\n",
       "0  French macaroon is so tasty      4\n",
       "1             work is horrible      3\n",
       "2                   I am upset      3\n",
       "3               throw the ball      1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I want to eat\\t</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he did not answer\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he got a very nice raise\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she got me a nice present\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      sentence  emoji\n",
       "0              I want to eat\\t      4\n",
       "1          he did not answer\\t      3\n",
       "2   he got a very nice raise\\t      2\n",
       "3  she got me a nice present\\t      2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate x,y,hot_encoding for test and train\n",
    "\n",
    "train_x = train.loc[:,'sentence'].to_numpy()\n",
    "train_y = train.loc[:,'emoji'].to_numpy()\n",
    "\n",
    "test_x = test.loc[:,'sentence'].to_numpy()\n",
    "test_y = test.loc[:,'emoji'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def one_hot_encoding(data,size):\n",
    "    data = np.eye(size)[data.reshape(-1)]\n",
    "    return data\n",
    "\n",
    "train_hot = one_hot_encoding(train_y, 5)\n",
    "test_hot = one_hot_encoding(test_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1671715138215,
     "user": {
      "displayName": " 专抓",
      "userId": "09515891463711318510"
     },
     "user_tz": -120
    },
    "id": "8V4Z-e355cUA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French macaroon is so tasty - \n",
      "work is horrible - \n",
      "I am upset - \n",
      "throw the ball - \n",
      "Good joke - \n",
      "what is your favorite baseball game - \n",
      "I cooked meat - \n",
      "stop messing around - \n",
      "I want chinese food - \n",
      "Let us go play baseball - \n"
     ]
    }
   ],
   "source": [
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",   \n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}\n",
    "\n",
    "def label_to_emoji(label):\n",
    "    # Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "    # debug prints: print(emoji.emojize(emoji_dictionary[str(label)], language='alias'))\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], language='alias')\n",
    "\n",
    "for i,row in train[:10].iterrows():\n",
    "    # debug print:\n",
    "    print('%s - %s' %(row['sentence'], label_to_emoji(row['emoji'])))\n",
    "    label_to_emoji(row['emoji'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xH9Up7pB5cUB"
   },
   "source": [
    "## Help functions for word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmkwCoop5cUC"
   },
   "source": [
    "The following functions will help you conver words and sentences to vectors and matrixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1671715140870,
     "user": {
      "displayName": " 专抓",
      "userId": "09515891463711318510"
     },
     "user_tz": -120
    },
    "id": "NmzokL7m5cUC"
   },
   "outputs": [],
   "source": [
    "# A function that obtains vector representations for words. Each word is represented by vector with size 50.\n",
    "# words_to_index is a dictionary that maps word into indexes - every word has a number. 'banana' --> 67752\n",
    "# index_to_words is a dictionary that maps indexes into indexes - every index has a matching word. 344429 --> 'strawberry'\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r',encoding='UTF-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1671715166978,
     "user": {
      "displayName": " 专抓",
      "userId": "09515891463711318510"
     },
     "user_tz": -120
    },
    "id": "K09LOc6i5cUD"
   },
   "outputs": [],
   "source": [
    "# load word embeddings and create word_to_index and index_to_word dictionaries\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1671715187308,
     "user": {
      "displayName": " 专抓",
      "userId": "09515891463711318510"
     },
     "user_tz": -120
    },
    "id": "0IV7R94O5cUD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector embedding of banana is: [-0.25522  -0.75249  -0.86655   1.1197    0.12887   1.0121   -0.57249\n",
      " -0.36224   0.44341  -0.12211   0.073524  0.21387   0.96744  -0.068611\n",
      "  0.51452  -0.053425 -0.21966   0.23012   1.043    -0.77016  -0.16753\n",
      " -1.0952    0.24837   0.20019  -0.40866  -0.48037   0.10674   0.5316\n",
      "  1.111    -0.19322   1.4768   -0.51783  -0.79569   1.7971   -0.33392\n",
      " -0.14545  -1.5454    0.0135    0.10684  -0.30722  -0.54572   0.38938\n",
      "  0.24659  -0.85166   0.54966   0.82679  -0.68081  -0.77864  -0.028242\n",
      " -0.82872 ]\n",
      "The index of the word 'tree' is: 364528\n",
      "The word matcing the index 173081 is: happy\n"
     ]
    }
   ],
   "source": [
    "# visualization: \n",
    "\n",
    "print(f\"The vector embedding of banana is: {word_to_vec_map['banana']}\")\n",
    "print(f\"The index of the word 'tree' is: {word_to_index['tree']}\")\n",
    "print(f\"The word matcing the index 173081 is: {index_to_word[173081]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1671715180393,
     "user": {
      "displayName": " 专抓",
      "userId": "09515891463711318510"
     },
     "user_tz": -120
    },
    "id": "jXzo-oDe5cUE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[176468., 389938.,      0.,      0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A function that translates the sentences vectors of word indexes --> I love you --> [185457,226278,394475]\n",
    "# the function uses padding of the longes sentence in the train set, so I love you --> [185457,226278,394475,0,0,0,0,0,0,0]\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            X_indices[i, j] = int(word_to_index[w])\n",
    "            j = j + 1\n",
    "    return X_indices\n",
    "\n",
    "sentences_to_indices(np.array(['hello world']),word_to_index,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "dv-rW3DU5cUE"
   },
   "outputs": [],
   "source": [
    "# function that maps all the word indexes to their vectors embedding. \n",
    "# the embedding function is in shape (400000, 50) - each word is a vector in size 50.\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len = len(word_to_index) + 1  #word index begin with 1,plus 1 for padding 0\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0] # the size of embedding of each word\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    return emb_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqJqHN8A5cUF"
   },
   "source": [
    "## Train and test data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5gF3q5q5cUF"
   },
   "source": [
    "The models that you will build will get the sentences as their vector representations - i.e., the sentences_to_indices() function output. \n",
    "\n",
    "Therefore, you need to:\n",
    "* Transform the data to the right form using the above functions\n",
    "* Transform the data and lables to tensors\n",
    "* If needed, create train and test data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "\n",
    "# find max len\n",
    "train_max_len = train['sentence'].apply(len).max()\n",
    "test_max_len = test['sentence'].apply(len).max()\n",
    "max_len = max(train_max_len,test_max_len) + 10\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "HecBSiI45cUF"
   },
   "outputs": [],
   "source": [
    "# sentences_to_indices\n",
    "train_indices = sentences_to_indices(train_x, word_to_index, max_len)\n",
    "test_indices = sentences_to_indices(test_x, word_to_index, max_len)\n",
    "\n",
    "# train tensors\n",
    "train_x_tensor = torch.from_numpy(train_indices)\n",
    "train_y_tensor = torch.from_numpy(train_y)\n",
    "train_tensor = torch.utils.data.TensorDataset(train_x_tensor, train_y_tensor)\n",
    "\n",
    "# test tensors\n",
    "test_x_tensor = torch.from_numpy(test_indices)\n",
    "test_y_tensor = torch.from_numpy(test_y)\n",
    "test_tensor = torch.utils.data.TensorDataset(test_x_tensor, test_y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_tensor, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_tensor, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BMgodE05cUF"
   },
   "source": [
    "# First model - regular neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqP7DJHY5cUF"
   },
   "source": [
    "Build a neural network model that gets:\n",
    "\n",
    "1. The vocabulary size\n",
    "2. Embedding dimention - the length of every embedding vector\n",
    "3. Pretrained embedding weights - the embedding matrix \n",
    "                                       \n",
    "and returns: \n",
    "1. 5 dimention vector with the scores of every emoji.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Then train the model and plot loss vs. epoch for train and test set. \n",
    "\n",
    "Show the results on 5 new sentences.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "You can use the added model as your base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim,pretrained_weight):\n",
    "        super(NN_Model,self).__init__()\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim) # stores embeddings of a fixed dictionary and size\n",
    "        self.word_embeds.weight.data.copy_(torch.from_numpy(pretrained_weight)) # place the pretrained weights to the embedding function\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32 ,5),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.word_embeds(x) \n",
    "        out = out[:,-1,:]\n",
    "        out = self.layers(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN_Model(\n",
      "  (word_embeds): Embedding(400001, 50)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=5, bias=True)\n",
      "    (5): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# prepare param\n",
    "vocab_len = len(word_to_index) + 1\n",
    "embedded_matrix = pretrained_embedding_layer(word_to_vec_map,word_to_index)\n",
    "\n",
    "# create neural network model object\n",
    "nn_model = NN_Model(vocab_len, 50, embedded_matrix)\n",
    "\n",
    "# sanity check \n",
    "print(nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in nn_model.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-16e4e0d03733>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# define loss function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# define param + settings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mopt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# define loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# define param + settings\n",
    "opt1 = torch.optim.Adam(nn_model.parameters(), lr = 0.0005)\n",
    "min_valid = np.Inf\n",
    "loss_training = []\n",
    "loss_validation = []\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "epochs = 300\n",
    "\n",
    "# train\n",
    "for epoch in range(epochs):\n",
    "    nn_model.train()\n",
    "    loss_train = 0 \n",
    "    accuracy_train = 0 \n",
    "    for data,label in train_loader:\n",
    "        data = data.long()\n",
    "        label = label.long()\n",
    "        nn_model.zero_grad()\n",
    "        data = Variable(data)\n",
    "        label = Variable(label)\n",
    "        output = nn_model(data)\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        opt1.step()\n",
    "        loss_train += loss.item()\n",
    "        label = label.cpu().numpy()\n",
    "        output = output.cpu().detach().numpy()\n",
    "        for i in range(len(data)):\n",
    "            model_label = np.argmax(output[i])\n",
    "            if model_label == label[i]:\n",
    "                accuracy_train += 1\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            nn_model.eval()\n",
    "            accuracy_test = 0 \n",
    "            loss_valid = 0\n",
    "            for data1,label1 in test_loader:\n",
    "                data1 = Variable(data1.long())\n",
    "                label1 = Variable(label1.long())\n",
    "                pred = nn_model(data1)\n",
    "                loss = loss_function(pred,label1)\n",
    "                loss_valid += loss.item()\n",
    "                label1 = label1.cpu().numpy()\n",
    "                pred = pred.cpu().numpy()\n",
    "                for i in range(len(data1)):\n",
    "                    model_label = np.argmax(pred[i])\n",
    "                    if model_label == label1[i]:\n",
    "                        accuracy_test += 1\n",
    "            accuracy_train = accuracy_train/len(train_x_tensor)\n",
    "            accuracy_test = accuracy_test/len(test_x_tensor)\n",
    "    loss_training.append(loss_train)\n",
    "    acc_train.append(accuracy_train)\n",
    "    loss_validation.append(loss_valid)\n",
    "    acc_valid.append(accuracy_test)\n",
    "\n",
    "    # performance prints\n",
    "    if (epoch+1)%50==0:\n",
    "        print('Epoch # %s' %(epoch+1))\n",
    "        print('training accuracy - %s' %(accuracy_train*100.0))\n",
    "        print('training loss - %s' %(loss_train))\n",
    "        print('validation accuracy - %s' %(accuracy_test*100.0))\n",
    "        print('validation loss - %s' %(loss_valid))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_training' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ac037a672b5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mplot_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_training' is not defined"
     ]
    }
   ],
   "source": [
    "# plot loss function\n",
    "\n",
    "def plot_loss(train_loss, test_loss, train_acc, test_acc, epochs):\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(range(1,epochs), train_loss, color='orange', label='train')\n",
    "    plt.plot(range(1,epochs), test_loss, color='blue', label='test')\n",
    "    plt.title('Loss per num epochs')\n",
    "    plt.xlabel('Num Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_loss(loss_training, loss_validation, acc_train, acc_valid, epochs+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy function\n",
    "\n",
    "def plot_accuracy(train_loss, test_loss, train_acc, test_acc, epochs):\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(range(1,epochs), train_acc, color='orange', label='train')\n",
    "    plt.plot(range(1,epochs), test_acc, color='blue', label='test')\n",
    "    plt.title('Accuracy per num epochs')\n",
    "    plt.xlabel('Num Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_accuracy(loss_training, loss_validation, acc_train, acc_valid, epochs+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new setences\n",
    "\n",
    "sentences = np.array(['I love apple', 'I love banana', 'cook me dinner', 'we went to a baseball game yesterday' , 'why so grumpy?'])\n",
    "\n",
    "sentences_indices = sentences_to_indices(sentences, word_to_index, 10)\n",
    "sentences_indices = torch.from_numpy(sentences_indices)\n",
    "sentences_indices = Variable(sentences_indices.long())\n",
    "\n",
    "prediction = nn_model(sentences_indices)\n",
    "\n",
    "for sentence in range(len(sentences)):\n",
    "    sentence_emoji = np.argmax(prediction.detach().numpy()[sentence])\n",
    "    print(sentences_indices[sentence], label_to_emoji(sentence_emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loawPDFv5cUG"
   },
   "source": [
    "# Second model - neural network with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLDhHs3Y5cUG"
   },
   "source": [
    "Build a neural network + RNN model that gets the vocabulary size, embedding dimention and pretrained embedding weights, and returns a 5 dimention vector with the scores of every emoji.\n",
    "\n",
    "Then train the model and plot loss vs. epoch for train and test set.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "\n",
    "---\n",
    "\n",
    "You can use the added model as your base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim,pretrained_weight):\n",
    "        super(RNN_Model,self).__init__()\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim) # stores embeddings of a fixed dictionary and size\n",
    "        self.word_embeds.weight.data.copy_(torch.from_numpy(pretrained_weight)) # place the pretrained weights to the embedding function\n",
    "        self.rnn = nn.LSTM(embedding_dim, 64, 3, batch_first=True, dropout=1/3)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 5),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x,h):\n",
    "        out = self.word_embeds(x)\n",
    "        out, _ = self.rnn(out,h)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.layers(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_Model(\n",
      "  (word_embeds): Embedding(400001, 50)\n",
      "  (rnn): LSTM(50, 64, num_layers=3, batch_first=True, dropout=0.3333333333333333)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=5, bias=True)\n",
      "    (3): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# prepare param\n",
    "vocab_len = len(word_to_index) + 1\n",
    "embedded_matrix = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "\n",
    "# create RNN model object\n",
    "rnn_model = RNN_Model(vocab_len, 50, embedded_matrix)\n",
    "\n",
    "# sanity check \n",
    "print(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in rnn_model.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 50\n",
      "training accuracy - 30.601092896174865\n",
      "training loss - 9.271894216537476\n",
      "validation accuracy - 32.142857142857146\n",
      "validation loss - 3.0696232318878174\n",
      "\n",
      "Epoch # 100\n",
      "training accuracy - 30.601092896174865\n",
      "training loss - 9.263789176940918\n",
      "validation accuracy - 32.142857142857146\n",
      "validation loss - 3.0795340538024902\n",
      "\n",
      "Epoch # 150\n",
      "training accuracy - 30.601092896174865\n",
      "training loss - 9.2770414352417\n",
      "validation accuracy - 32.142857142857146\n",
      "validation loss - 3.057257652282715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# define param + settings\n",
    "opt2 = torch.optim.Adam(rnn_model.parameters(), lr = 0.0005)\n",
    "min_valid = np.Inf\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "loss_training = []\n",
    "loss_validation = []\n",
    "num_epochs = 300\n",
    "\n",
    "\n",
    "# train\n",
    "for epoch in range(num_epochs):\n",
    "    rnn_model.train()\n",
    "    loss_train = 0 \n",
    "    accuracy_train = 0 \n",
    "    for data,label in train_loader:\n",
    "        data = data.long()\n",
    "        label = label.long()\n",
    "        rnn_model.zero_grad()\n",
    "        states = (Variable(torch.zeros(3, len(data), 64)), Variable(torch.zeros(3, len(data), 64)))\n",
    "        data = Variable(data)\n",
    "        label = Variable(label)\n",
    "        output = rnn_model(data, states)\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        opt2.step()\n",
    "        loss_train +=loss.item()\n",
    "        label = label.cpu().numpy()\n",
    "        output = output.cpu().detach().numpy()\n",
    "        for i in range(len(data)):\n",
    "            model_label = np.argmax(output[i])\n",
    "            if model_label == label[i]:\n",
    "                accuracy_train += 1\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            rnn_model.eval()\n",
    "            accuracy_test = 0 \n",
    "            loss_valid = 0\n",
    "            for data1,label1 in test_loader:\n",
    "                data1 = Variable(data1.long())\n",
    "                label1 = Variable(label1.long())\n",
    "                states1 = (Variable(torch.zeros(3, len(data1), 64)), Variable(torch.zeros(3, len(data1), 64)))\n",
    "                pred = rnn_model(data1, states1)\n",
    "                loss = loss_function(pred,label1)\n",
    "                loss_valid += loss.item()\n",
    "                label1 = label1.cpu().numpy()\n",
    "                pred = pred.cpu().numpy()\n",
    "                for i in range(len(data1)):\n",
    "                    model_label = np.argmax(pred[i])\n",
    "                    if model_label == label1[i]:\n",
    "                        accuracy_test += 1\n",
    "            accuracy_train = accuracy_train/len(train_x_tensor)\n",
    "            accuracy_test = accuracy_test/len(test_x_tensor)\n",
    "\n",
    "    loss_training.append(loss_train)\n",
    "    acc_train.append(accuracy_train)\n",
    "    loss_validation.append(loss_valid)\n",
    "    acc_valid.append(accuracy_test)\n",
    "\n",
    "    if (epoch+1)%50==0:\n",
    "        print('Epoch # %s' %(epoch+1))\n",
    "        print('training accuracy - %s' %(accuracy_train*100.0))\n",
    "        print('training loss - %s' %(loss_train))\n",
    "        print('validation accuracy - %s' %(accuracy_test*100.0))\n",
    "        print('validation loss - %s' %(loss_valid))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plot_loss(loss_training, loss_validation, acc_train, acc_valid, epochs+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "plot_accuracy(loss_training, loss_validation, acc_train, acc_valid, epochs+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new setences\n",
    "\n",
    "sentences = np.array(['I love apple', 'I love banana', 'cook me dinner', 'we went to a baseball game yesterday' , 'why so grumpy?'])\n",
    "\n",
    "sentences_indices = sentences_to_indices(sentences, word_to_index, 10)\n",
    "sentences_indices = torch.from_numpy(sentences_indices)\n",
    "sentences_indices = Variable(sentences_indices.long())\n",
    "\n",
    "prediction = rnn_model(sentences_indices)\n",
    "\n",
    "for sentence in range(len(sentences)):\n",
    "    sentence_emoji = np.argmax(prediction.detach().numpy()[sentence])\n",
    "    print(sentences_indices[sentence], label_to_emoji(sentence_emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyGh1SVB78ur"
   },
   "source": [
    "# Third model - neural network with transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ko-FP1978ur"
   },
   "source": [
    "Build a neural network + transformer model that gets the vocabulary size, embedding dimention and pretrained embedding weights, and returns a 5 dimention vector with the scores of every emoji.\n",
    "\n",
    "Then train the model and plot loss vs. epoch for train and test set.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "You can use the added model as your base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SbO1izqW78ur"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "# train tensors\n",
    "train_x_tensor = torch.from_numpy(train_indices)\n",
    "train_y_tensor = torch.from_numpy(train_hot)\n",
    "train_tensor = torch.utils.data.TensorDataset(train_x_tensor, train_y_tensor)\n",
    "\n",
    "# test tensors\n",
    "test_x_tensor = torch.from_numpy(test_indices)\n",
    "test_y_tensor = torch.from_numpy(test_hot)\n",
    "test_tensor = torch.utils.data.TensorDataset(test_x_tensor, test_y_tensor)\n",
    "\n",
    "# dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_tensor, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_tensor, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1671715302552,
     "user": {
      "displayName": " 专抓",
      "userId": "09515891463711318510"
     },
     "user_tz": -120
    },
    "id": "O_HZJmZN8Shu"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "F0G4Y_Fq78us"
   },
   "outputs": [],
   "source": [
    "class Transformer_model(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size, embedding_dim, pretrained_weight):\n",
    "        super(Transformer_model,self).__init__()\n",
    "        transform_dim = 100\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeds.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        self.emb2transformer = nn.Linear(embedding_dim, transform_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embedding_dim)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model=transform_dim, nhead=5, num_encoder_layers=2, num_decoder_layers=1, dropout=1/3)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=transform_dim, num_heads=5)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(transform_dim, 5),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.word_embeds(x)\n",
    "        out += self.pos_encoding(out)\n",
    "        out = self.emb2transformer(out)\n",
    "        out = self.transformer(out,out)\n",
    "        out = torch.sum(out, axis = 1)\n",
    "        out = self.layers(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer_model(\n",
      "  (word_embeds): Embedding(400001, 50)\n",
      "  (emb2transformer): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (pos_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.3333333333333333, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
      "          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.3333333333333333, inplace=False)\n",
      "          (dropout2): Dropout(p=0.3333333333333333, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.3333333333333333, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
      "          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.3333333333333333, inplace=False)\n",
      "          (dropout2): Dropout(p=0.3333333333333333, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=100, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.3333333333333333, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=100, bias=True)\n",
      "          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.3333333333333333, inplace=False)\n",
      "          (dropout2): Dropout(p=0.3333333333333333, inplace=False)\n",
      "          (dropout3): Dropout(p=0.3333333333333333, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=5, bias=True)\n",
      "    (1): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# prepare param\n",
    "vocab_len = len(word_to_index) + 1\n",
    "embedded_matrix = pretrained_embedding_layer(word_to_vec_map,word_to_index)\n",
    "\n",
    "# create neural network model object\n",
    "transformers_nn_model = Transformer_model(vocab_len, 50, embedded_matrix)\n",
    "\n",
    "# sanity check \n",
    "print(transformers_nn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# define param + settings\n",
    "opt3 = torch.optim.Adam(transformers_nn_model.parameters(), lr = 0.0005)\n",
    "min_valid = np.Inf\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "loss_training = []\n",
    "loss_validation = []\n",
    "epochs = 300\n",
    "\n",
    "\n",
    "# train\n",
    "for epoch in range(epochs):\n",
    "    transformers_nn_model.train()\n",
    "    loss_train = 0 \n",
    "    accuracy_train = 0 \n",
    "    for data,label in train_loader:\n",
    "        data = data.long()\n",
    "        # label = label.long()\n",
    "        data = Variable(data)\n",
    "        label = Variable(label)\n",
    "        output = transformers_nn_model(data)\n",
    "        opt3.zero_grad()\n",
    "        loss = loss_function(output, label.float())\n",
    "        loss.backward()\n",
    "        opt3.step()\n",
    "        loss_train += loss.item()\n",
    "        label = label.cpu().numpy()\n",
    "        output = output.cpu().detach().numpy()\n",
    "        for i in range(len(data)):\n",
    "            model_label = np.argmax(output[i])\n",
    "            true_label = np.where(label[i] == 1)[0][0]\n",
    "            if model_label == true_label:\n",
    "              accuracy_train += 1\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            transformers_nn_model.eval()\n",
    "            accuracy_test = 0 \n",
    "            loss_valid = 0\n",
    "            for data1,label1 in test_loader:\n",
    "                data1 = Variable(data1.long())\n",
    "                pred = transformers_nn_model(data1)\n",
    "                loss = loss_function(pred,label1)\n",
    "                loss_valid += loss.item()\n",
    "                label1 = label1.cpu().numpy()\n",
    "                pred = pred.cpu().numpy()\n",
    "                for i in range(len(data)):\n",
    "                    model_label = np.argmax(output[i])\n",
    "                    true_label = np.where(label1[i] == 1)[0][0]\n",
    "                    if model_label == true_label:\n",
    "                        accuracy_test += 1\n",
    "\n",
    "            accuracy_train = accuracy_train/len(train_x_tensor)\n",
    "            accuracy_test = accuracy_test/len(test_x_tensor)\n",
    "        \n",
    "    loss_training.append(loss_train)\n",
    "    acc_train.append(accuracy_train)\n",
    "    loss_validation.append(loss_valid)\n",
    "    acc_valid.append(accuracy_test)\n",
    "    \n",
    "    if (epoch+1)%50==0:\n",
    "        print('Epoch # %s' %(epoch+1))\n",
    "        print('training accuracy - %s' %(accuracy_train*100.0))\n",
    "        print('training loss - %s' %(loss_train))\n",
    "        print('validation accuracy - %s' %(accuracy_test*100.0))\n",
    "        print('validation loss - %s' %(loss_valid))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(loss_training, loss_validation, acc_train, acc_valid, epochs+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(loss_training, loss_validation, acc_train, acc_valid, epochs+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ApF5nFs5cUH"
   },
   "source": [
    "# Compare between the models - who had the best results? Try to explain why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxEIIjmS5cUH"
   },
   "source": [
    "RNN showed the best results.\n",
    "\n",
    "RNNs fit for identifying a sentence to an emoji because they have the ability to retain information from previous input sequences. but, this ability might lead to overfitting.\n",
    "\n",
    "A regular network can't have the ability to retain information from previous input sequences, so it shows worst results. however, it's less prone to overfitting.\n",
    "\n",
    "transformer networks also showed bad results. This may be beacuse identifying an emoji from a sentence depends on emotion and this may not be captured by the self-attention mechanism transformer network. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
